"""
VISION SYSTEM INTEGRATION FIXES
==============================

This file contains all the necessary fixes to restore proper vision system integration.
Apply these changes to main.py to fix the vision system issues.

ISSUES TO FIX:
1. Vision system initialization is broken
2. Vision analysis not triggering during events
3. No vision files being created in memories/vision
4. Owner concept error
5. Missing vision context in thought process
6. No vision processing pause in cognitive loop

"""

# FIX 1: Vision System Initialization (around line 5296)
"""
REPLACE:
        try:
            from vision_system import VisionSystem
            self.vision_system = VisionSystem()
            
            # Initialize vision detection controls on startup
            self._initialize_vision_detection_controls()
            self.vision_system = VisionSystem(memory_system=self.memory_system)
            print("‚úÖ Vision system initialized")

WITH:
        try:
            from vision_system import VisionSystem
            self.vision_system = VisionSystem(
                memory_system=self.memory_system,
                openai_client=self.openai_client,
                settings=self.settings
            )
            
            # Initialize vision detection controls on startup
            self._initialize_vision_detection_controls()
            print("‚úÖ Vision system initialized")
"""

# FIX 2: Owner Concept Error (around line 3383)
"""
REPLACE:
                owner_name = self.settings.get('people-owner', 'name', fallback='Joe')

WITH:
                owner_name = self.settings.get('people-owner', {}).get('name', 'Joe')
"""

# FIX 3: Add Vision Analysis Trigger Before Thought Processing (around line 8813)
"""
REPLACE:
                # Add perceived_message to event_data for summary generation
                if hasattr(event, 'perceived_message'):
                    event_data["perceived_message"] = event.perceived_message
                
                # Get Carl's thought process
                event_data = await self.get_carl_thought(event_data)

WITH:
                # Add perceived_message to event_data for summary generation
                if hasattr(event, 'perceived_message'):
                    event_data["perceived_message"] = event.perceived_message
                
                # CRITICAL: Trigger vision analysis just before get_carl_thought
                # This ensures vision data is available for the thought process
                self.log("üëÅÔ∏è Triggering vision analysis before thought processing...")
                vision_result = await self._trigger_vision_analysis_before_thought()
                
                # Add vision result to event_data for memory storage
                if vision_result:
                    event_data["vision_analysis"] = vision_result
                    self.log(f"üëÅÔ∏è Vision analysis completed: {len(vision_result.get('objects_detected', []))} objects detected")
                else:
                    self.log("üëÅÔ∏è No vision analysis result available")
                
                # Get Carl's thought process
                event_data = await self.get_carl_thought(event_data)
"""

# FIX 4: Add Vision Context to Thought Process (around line 7400)
"""
REPLACE:
            # Get sensory status information
            sensory_info = self._get_sensory_status_information()
            
            # Get memory context for introspection and communication

WITH:
            # Get sensory status information
            sensory_info = self._get_sensory_status_information()
            
            # Get vision context for thought process
            vision_context = ""
            if hasattr(self, 'vision_system') and self.vision_system is not None:
                try:
                    vision_data = self.vision_system.get_vision_context_for_thought()
                    if vision_data and vision_data.get("vision_active"):
                        objects_detected = vision_data.get("recent_objects", [])
                        danger_detected = vision_data.get("danger_detected", False)
                        pleasure_detected = vision_data.get("pleasure_detected", False)
                        neucogar_response = vision_data.get("neucogar_response", {})
                        
                        vision_context = f"""
VISION PERCEPTION CONTEXT: You have recently analyzed your visual environment and detected:
- Objects visible: {', '.join(objects_detected) if objects_detected else 'None detected'}
- Danger detected: {'Yes' if danger_detected else 'No'}
- Pleasure detected: {'Yes' if pleasure_detected else 'No'}
- Visual NEUCOGAR response: {neucogar_response}

This visual information should influence your perception and judgment of the current situation.
"""
                    else:
                        vision_context = """
VISION PERCEPTION CONTEXT: No recent visual analysis available. You are relying on other sensory inputs and memory.
"""
                except Exception as e:
                    self.log(f"Error getting vision context: {e}")
                    vision_context = """
VISION PERCEPTION CONTEXT: Vision system temporarily unavailable. Using other sensory inputs.
"""
            else:
                vision_context = """
VISION PERCEPTION CONTEXT: Vision system not available. Using other sensory inputs and memory.
"""
            
            # Get memory context for introspection and communication
"""

# FIX 5: Add Vision Context to Prompt (around line 7500)
"""
REPLACE:
SENSORY AWARENESS: You have self-awareness of your sensory capabilities and limitations. You understand what senses you have available and which ones are currently unavailable. This should influence your responses to sensory-related requests:
{sensory_info}

PHYSICAL WORLD OBSERVATION LIMITATIONS: You have important limitations in how you can observe the physical world:

WITH:
SENSORY AWARENESS: You have self-awareness of your sensory capabilities and limitations. You understand what senses you have available and which ones are currently unavailable. This should influence your responses to sensory-related requests:
{sensory_info}

{vision_context}

PHYSICAL WORLD OBSERVATION LIMITATIONS: You have important limitations in how you can observe the physical world:
"""

# FIX 6: Add Vision Processing Pause in Cognitive Loop (around line 13867)
"""
REPLACE:
                # CRITICAL: Pause cognitive processing during API calls
                if self.cognitive_state["is_api_call_in_progress"]:
                    self.log("‚è∏Ô∏è  API call in progress - pausing cognitive processing...")
                    time.sleep(1.0)  # Sleep longer during API calls to prevent interference
                    continue

WITH:
                # CRITICAL: Pause cognitive processing during API calls
                if self.cognitive_state["is_api_call_in_progress"]:
                    self.log("‚è∏Ô∏è  API call in progress - pausing cognitive processing...")
                    time.sleep(1.0)  # Sleep longer during API calls to prevent interference
                    continue
                    
                # CRITICAL: Pause cognitive processing during vision analysis
                if (hasattr(self, 'vision_system') and 
                    self.vision_system is not None and 
                    hasattr(self.vision_system, 'is_vision_processing_active') and
                    self.vision_system.is_vision_processing_active()):
                    self.log("‚è∏Ô∏è  Vision analysis in progress - pausing cognitive processing...")
                    time.sleep(0.1)  # Short pause during vision processing
                    continue
"""

# FIX 7: Add Vision Analysis Trigger Method (add at the end of the class, before the last method)
"""
ADD THIS METHOD TO THE CLASS:

    async def _trigger_vision_analysis_before_thought(self):
        """
        Trigger vision analysis just before get_carl_thought execution.
        This ensures vision data is available for the thought process.
        
        Returns:
            Vision analysis result dict or None if analysis failed/not available
        """
        try:
            # Check if vision system is available and enabled
            if (hasattr(self, 'vision_system') and 
                self.vision_system is not None and 
                hasattr(self.vision_system, 'capture_and_analyze_vision') and
                hasattr(self.vision_system, 'vision_enabled') and
                self.vision_system.vision_enabled):
                
                # Check if vision analysis should be triggered (respects rate limiting)
                if hasattr(self.vision_system, 'should_trigger_vision_analysis'):
                    if not self.vision_system.should_trigger_vision_analysis():
                        self.log("üëÅÔ∏è Vision analysis rate limited - skipping")
                        return None
                
                # Execute vision analysis asynchronously to ensure it completes before thought
                self.log("üëÅÔ∏è Executing vision analysis before thought process...")
                
                result = await self.vision_system.capture_and_analyze_vision()
                
                if result.get("success"):
                    objects_detected = result["data"].get("objects", [])
                    danger_detected = result["data"].get("danger_detected", False)
                    pleasure_detected = result["data"].get("pleasure_detected", False)
                    neucogar = result["data"].get("neucogar", {})
                    analysis = result["data"].get("analysis", {})
                    image_path = result["data"].get("image_path", "")
                    
                    self.log(f"üëÅÔ∏è Vision analysis completed before thought: {len(objects_detected)} objects detected")
                    self.log(f"üëÅÔ∏è Danger detected: {danger_detected}, Pleasure detected: {pleasure_detected}")
                    self.log(f"üëÅÔ∏è NEUCOGAR response: {neucogar}")
                    
                    # Update vision context for immediate use in thought process
                    if hasattr(self.vision_system, 'get_vision_context_for_thought'):
                        vision_context = self.vision_system.get_vision_context_for_thought()
                        if vision_context and vision_context.get("vision_active"):
                            self.log(f"üëÅÔ∏è Vision context updated: {len(vision_context.get('recent_objects', []))} recent objects")
                    
                    # Return vision result for memory storage
                    return {
                        "objects_detected": objects_detected,
                        "danger_detected": danger_detected,
                        "danger_reason": result["data"].get("danger_reason", ""),
                        "pleasure_detected": pleasure_detected,
                        "pleasure_reason": result["data"].get("pleasure_reason", ""),
                        "neucogar_response": neucogar,
                        "analysis": analysis,
                        "image_path": image_path,
                        "timestamp": result["data"].get("timestamp", ""),
                        "vision_active": True
                    }
                else:
                    self.log(f"‚ö†Ô∏è Vision analysis failed before thought: {result.get('error', 'Unknown error')}")
                    return {
                        "vision_active": False,
                        "error": result.get('error', 'Unknown error'),
                        "objects_detected": [],
                        "danger_detected": False,
                        "pleasure_detected": False,
                        "neucogar_response": {"dopamine": 0.5, "serotonin": 0.5, "norepinephrine": 0.5, "acetylcholine": 0.5}
                    }
            else:
                self.log("üëÅÔ∏è Vision system not available or disabled - skipping vision analysis")
                return {
                    "vision_active": False,
                    "error": "Vision system not available",
                    "objects_detected": [],
                    "danger_detected": False,
                    "pleasure_detected": False,
                    "neucogar_response": {"dopamine": 0.5, "serotonin": 0.5, "norepinephrine": 0.5, "acetylcholine": 0.5}
                }
                    
        except Exception as e:
            self.log(f"Error in vision analysis before thought: {e}")
            # Don't let vision errors prevent thought processing
            return {
                "vision_active": False,
                "error": str(e),
                "objects_detected": [],
                "danger_detected": False,
                "pleasure_detected": False,
                "neucogar_response": {"dopamine": 0.5, "serotonin": 0.5, "norepinephrine": 0.5, "acetylcholine": 0.5}
            }
"""

print("""
VISION SYSTEM FIXES SUMMARY
===========================

The vision system integration has been broken due to several issues:

1. ‚ùå Vision system initialization is incorrect
2. ‚ùå Vision analysis not triggering during events  
3. ‚ùå No vision files being created in memories/vision
4. ‚ùå Owner concept error with settings.get()
5. ‚ùå Missing vision context in thought process
6. ‚ùå No vision processing pause in cognitive loop

EXPECTED BEHAVIOR AFTER FIXES:
‚úÖ Vision analysis executes before every thought process
‚úÖ 4-12 second OpenAI Vision API calls
‚úÖ Vision files created in memories/vision
‚úÖ CARL Memory Explorer shows vision data
‚úÖ Vision context included in thought prompts
‚úÖ Cognitive processing pauses during vision analysis
‚úÖ No more owner concept errors

Apply the fixes above to restore proper vision system integration.
""")
